# -*- coding: utf-8 -*-
"""Computer Vision-based Waste Segregation System for Smart Cities

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EfKaVelnncdr-f1fqCoMRuP7YAXwF4iy
"""

from google.colab import drive
import os

# Mount Google Drive
drive.mount('/content/drive')

# Explore the directory structure
def explore_drive_structure():
    print("🔍 EXPLORING GOOGLE DRIVE STRUCTURE")
    print("=" * 50)

    # Check common locations
    possible_locations = [
        "/content/drive/MyDrive/DATA.",
        "/content/drive/MyDrive/data",
        "/content/drive/MyDrive/DATASET",
        "/content/drive/MyDrive/WasteData",
        "/content/drive/MyDrive",
    ]

    for location in possible_locations:
        if os.path.exists(location):
            print(f"✅ Found: {location}")
            # List contents
            try:
                items = os.listdir(location)
                print(f"   Contents: {items}")

                # Check if this contains our data folders
                for item in items:
                    item_path = os.path.join(location, item)
                    if os.path.isdir(item_path):
                        sub_items = os.listdir(item_path)
                        print(f"   📁 {item}/: {sub_items}")
            except Exception as e:
                print(f"   ❌ Error reading: {e}")
        else:
            print(f"❌ Not found: {location}")

explore_drive_structure()

# Search for folders that might contain your dataset
def search_for_dataset_folders():
    print("\n🔍 SEARCHING FOR DATASET FOLDERS")
    print("=" * 50)

    search_locations = [
        "/content/drive/MyDrive/DATA./data",
        "/content/drive/MyDrive/DATA",
    ]

    target_folders = ['battery', 'plastic', 'paper', 'metal', 'glass', 'biological', 'trash', 'shoes', 'clothes', 'cardboard']

    for base_path in search_locations:
        if os.path.exists(base_path):
            print(f"\n📁 Searching in: {base_path}")

            # List all items in this directory
            all_items = os.listdir(base_path)
            print(f"All items here: {all_items}")

            # Check for our target folders
            found_folders = []
            for folder in target_folders:
                folder_path = os.path.join(base_path, folder)
                if os.path.exists(folder_path) and os.path.isdir(folder_path):
                    images = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
                    found_folders.append((folder, len(images)))
                    print(f"   ✅ Found {folder}: {len(images)} images")

            if found_folders:
                print(f"🎯 Found dataset folders: {found_folders}")
                return base_path, found_folders

    print("❌ No dataset folders found in common locations")
    return None, []

base_path, found_folders = search_for_dataset_folders()

# Specifically check what's in /content/drive/MyDrive/DATA
data_path = "/content/drive/MyDrive/DATA./data"
if os.path.exists(data_path):
    print(f"📁 Contents of {data_path}:")
    items = os.listdir(data_path)
    for item in items:
        item_path = os.path.join(data_path, item)
        if os.path.isdir(item_path):
            sub_items = os.listdir(item_path)
            print(f"   📁 {item}/: {len(sub_items)} items")
            # Show first few items
            for sub_item in sub_items[:3]:
                print(f"      - {sub_item}")
            if len(sub_items) > 3:
                print(f"      ... and {len(sub_items) - 3} more")
        else:
            print(f"   📄 {item}")
else:
    print(f"❌ {data_path} does not exist")

import os
import shutil
import random
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

def organize_waste_dataset_colab():
    """
    Colab-compatible version with corrected paths
    """

    # CORRECTED PATHS - using "raw data" instead of "raw_data"
    base_path = "/content/drive/MyDrive/DATA./data"
    raw_data_path = f"{base_path}/raw data"
    processed_path = f"{base_path}/processed"

    print(f"📁 Raw data path: {raw_data_path}")
    print(f"📁 Processed path: {processed_path}")

    # Check if paths exist
    if not os.path.exists(raw_data_path):
        print(f"❌ Raw data path not found: {raw_data_path}")
        # Let's see what actually exists
        if os.path.exists(base_path):
            print(f"📁 Contents of {base_path}:")
            for item in os.listdir(base_path):
                print(f"   - {item}")
        return 0

    if not os.path.exists(processed_path):
        print(f"❌ Processed path not found: {processed_path}")
        return 0

    # Category mapping
    category_mapping = {
        # 🔴 HAZARDOUS (Red Bin)
        'hazardous': ['battery'],

        # 🔵 RECYCLABLE (Blue Bin)
        'recyclable': ['plastic', 'paper', 'metal', 'glass', 'cardboard', 'clothes'],

        # 🟢 BIODEGRADABLE (Green Bin)
        'biodegradable': ['biological', 'trash', 'shoes']
    }

    # First, let's check what folders actually exist in raw data
    print("\n🔍 Checking raw data folders:")
    existing_folders = os.listdir(raw_data_path)
    print(f"Found folders: {existing_folders}")

    # Create processed directory structure
    splits = ['training', 'validation', 'testing']
    for split in splits:
        for category in category_mapping.keys():
            category_path = os.path.join(processed_path, split, category)
            os.makedirs(category_path, exist_ok=True)
            print(f"✅ Created folder: {category_path}")

    # Split ratios
    train_ratio = 0.70
    val_ratio = 0.15
    test_ratio = 0.15

    print("\n🚀 Starting dataset organization...")

    total_copied = 0

    # Process each waste category
    for target_category, source_folders in category_mapping.items():
        print(f"\n📂 Processing {target_category.upper()} category...")

        all_images = []

        # Collect all images from source folders
        for source_folder in source_folders:
            source_path = os.path.join(raw_data_path, source_folder)

            if not os.path.exists(source_path):
                print(f"   ⚠️  Folder '{source_folder}' not found at {source_path}, skipping...")
                continue

            # Get all image files
            image_files = []
            for f in os.listdir(source_path):
                if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):
                    image_files.append(f)

            print(f"   📁 {source_folder}: {len(image_files)} images")
            all_images.extend([(source_folder, f) for f in image_files])

        if not all_images:
            print(f"   ❌ No images found for {target_category}")
            continue

        # Shuffle images for random split
        random.shuffle(all_images)
        total_images = len(all_images)

        # Calculate split sizes
        train_count = int(total_images * train_ratio)
        val_count = int(total_images * val_ratio)
        test_count = total_images - train_count - val_count

        print(f"   📊 Total images: {total_images}")
        print(f"   📈 Split: Train({train_count}), Val({val_count}), Test({test_count})")

        # Split images
        train_images = all_images[:train_count]
        val_images = all_images[train_count:train_count + val_count]
        test_images = all_images[train_count + val_count:]

        # Copy images to respective folders
        copy_count = 0
        for split_name, split_images in zip(splits, [train_images, val_images, test_images]):
            for source_folder, image_file in split_images:
                src_path = os.path.join(raw_data_path, source_folder, image_file)
                dst_path = os.path.join(processed_path, split_name, target_category, image_file)

                try:
                    shutil.copy2(src_path, dst_path)
                    copy_count += 1
                except Exception as e:
                    print(f"   ❌ Error copying {image_file}: {e}")

        total_copied += copy_count
        print(f"   ✅ {target_category}: Copied {copy_count} images!")

    return total_copied

def verify_organization_colab():
    """
    Verify that the organization was successful
    """
    print("\n" + "="*50)
    print("🔍 VERIFYING ORGANIZATION")
    print("="*50)

    base_path = "/content/drive/MyDrive/DATA./data"
    processed_path = f"{base_path}/processed"

    splits = ['training', 'validation', 'testing']
    categories = ['hazardous', 'recyclable', 'biodegradable']

    total_images = 0

    for split in splits:
        print(f"\n📊 {split.upper()} Split:")
        for category in categories:
            category_path = os.path.join(processed_path, split, category)
            if os.path.exists(category_path):
                image_files = [f for f in os.listdir(category_path)
                             if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]
                image_count = len(image_files)
                print(f"   📁 {category}: {image_count} images")
                total_images += image_count

                # Show first 2 files as proof
                if image_files:
                    print(f"      Sample: {image_files[:2]}")
            else:
                print(f"   ❌ {category}: FOLDER NOT FOUND at {category_path}")

    print(f"\n🎯 TOTAL IMAGES ORGANIZED: {total_images}")
    return total_images

if __name__ == "__main__":
    print("🗑️ WASTE DATASET ORGANIZER - CORRECTED PATHS")
    print("="*50)

    # Run organization
    total_copied = organize_waste_dataset_colab()

    # Verify results
    total_verified = verify_organization_colab()

    if total_verified > 0:
        print(f"\n🎉 SUCCESS! Organized {total_verified} images!")
        print("\n📁 Your organized dataset is ready for model training! 🚀")
    else:
        print("\n❌ Organization failed! Please check the paths above.")

import os
import matplotlib.pyplot as plt
import numpy as np
from PIL import Image

def explore_organized_dataset():
    """
    Explore and visualize the organized dataset
    """
    base_path = "/content/drive/MyDrive/DATA./data"
    processed_path = f"{base_path}/processed"

    print("🔍 EXPLORING ORGANIZED DATASET")
    print("=" * 50)

    splits = ['training', 'validation', 'testing']
    categories = ['hazardous', 'recyclable', 'biodegradable']
    bin_colors = ['red', 'blue', 'green']

    # Count images in each category
    category_counts = {}

    plt.figure(figsize=(15, 5))

    for i, split in enumerate(splits):
        split_counts = []
        for category in categories:
            category_path = os.path.join(processed_path, split, category)
            if os.path.exists(category_path):
                image_count = len([f for f in os.listdir(category_path)
                                 if f.lower().endswith(('.png', '.jpg', '.jpeg'))])
                split_counts.append(image_count)

                if category not in category_counts:
                    category_counts[category] = 0
                category_counts[category] += image_count

        # Plot distribution for this split
        plt.subplot(1, 3, i+1)
        bars = plt.bar(categories, split_counts, color=bin_colors, alpha=0.7)
        plt.title(f'{split.upper()} Split\nTotal: {sum(split_counts)} images')
        plt.xticks(rotation=45)
        plt.ylabel('Number of Images')

        # Add value labels on bars
        for bar, count in zip(bars, split_counts):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,
                    str(count), ha='center', va='bottom')

    plt.tight_layout()
    plt.savefig(f'{base_path}/dataset_distribution.png')
    plt.show()

    # Print summary
    print("\n📊 DATASET SUMMARY:")
    print("=" * 30)
    total_images = sum(category_counts.values())
    print(f"Total Images: {total_images}")

    for category, count in category_counts.items():
        percentage = (count / total_images) * 100
        bin_color = "🔴" if category == "hazardous" else "🔵" if category == "recyclable" else "🟢"
        print(f"{bin_color} {category.upper()}: {count} images ({percentage:.1f}%)")

    # Check image dimensions
    print("\n📏 CHECKING IMAGE DIMENSIONS:")
    print("=" * 30)
    check_image_sizes(processed_path)

def check_image_sizes(processed_path):
    """
    Check the dimensions of images in the dataset
    """
    sample_sizes = []

    # Check a few images from each category
    for category in ['hazardous', 'recyclable', 'biodegradable']:
        category_path = os.path.join(processed_path, 'training', category)
        if os.path.exists(category_path):
            images = [f for f in os.listdir(category_path)
                     if f.lower().endswith(('.png', '.jpg', '.jpeg'))][:3]  # Check first 3

            for img_name in images:
                img_path = os.path.join(category_path, img_name)
                try:
                    with Image.open(img_path) as img:
                        sample_sizes.append(img.size)
                        print(f"   {category}/{img_name}: {img.size}")
                except Exception as e:
                    print(f"   ❌ Error reading {img_name}: {e}")

    if sample_sizes:
        avg_width = sum(size[0] for size in sample_sizes) / len(sample_sizes)
        avg_height = sum(size[1] for size in sample_sizes) / len(sample_sizes)
        print(f"\n📐 Average image size: {avg_width:.0f} x {avg_height:.0f} pixels")

if __name__ == "__main__":
    explore_organized_dataset()

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
import matplotlib.pyplot as plt
import numpy as np
import os

def train_waste_classifier():
    """
    Train a waste classification model using Transfer Learning
    """
    print("🚀 STARTING MODEL TRAINING")
    print("=" * 50)

    # Set paths
    base_path = "/content/drive/MyDrive/DATA./data"
    train_path = f"{base_path}/processed/training"
    val_path = f"{base_path}/processed/validation"

    # Model parameters
    IMG_SIZE = (224, 224)
    BATCH_SIZE = 32
    NUM_CLASSES = 3
    EPOCHS = 15

    print(f"📁 Training data: {train_path}")
    print(f"📁 Validation data: {val_path}")
    print(f"🖼️  Image size: {IMG_SIZE}")
    print(f"📦 Batch size: {BATCH_SIZE}")
    print(f"🎯 Number of classes: {NUM_CLASSES}")

    # Data Augmentation for Training
    train_datagen = ImageDataGenerator(
        rescale=1./255,
        rotation_range=20,
        width_shift_range=0.2,
        height_shift_range=0.2,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=True,
        fill_mode='nearest'
    )

    # Only rescaling for Validation
    val_datagen = ImageDataGenerator(rescale=1./255)

    # Create data generators
    print("\n📊 Creating data generators...")

    train_generator = train_datagen.flow_from_directory(
        train_path,
        target_size=IMG_SIZE,
        batch_size=BATCH_SIZE,
        class_mode='categorical',
        shuffle=True
    )

    val_generator = val_datagen.flow_from_directory(
        val_path,
        target_size=IMG_SIZE,
        batch_size=BATCH_SIZE,
        class_mode='categorical',
        shuffle=False
    )

    # Print class indices
    print(f"🎯 Class indices: {train_generator.class_indices}")

    # Build the Model using Transfer Learning
    print("\n🛠️ Building the model...")

    # Load pre-trained MobileNetV2
    base_model = MobileNetV2(
        weights='imagenet',
        include_top=False,
        input_shape=(224, 224, 3)
    )

    # Freeze base model layers initially
    base_model.trainable = False

    # Add custom layers on top
    inputs = keras.Input(shape=(224, 224, 3))
    x = base_model(inputs, training=False)
    x = GlobalAveragePooling2D()(x)
    x = Dense(128, activation='relu')(x)
    x = Dropout(0.3)(x)
    outputs = Dense(NUM_CLASSES, activation='softmax')(x)

    model = Model(inputs, outputs)

    # Compile the model
    model.compile(
        optimizer=Adam(learning_rate=0.001),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )

    print("✅ Model compiled successfully!")
    print(model.summary())

    # Callbacks
    early_stop = EarlyStopping(
        monitor='val_accuracy',
        patience=5,
        restore_best_weights=True
    )

    reduce_lr = ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.2,
        patience=3,
        min_lr=0.0001
    )

    # Train the model
    print("\n🎯 Starting training...")

    history = model.fit(
        train_generator,
        steps_per_epoch=train_generator.samples // BATCH_SIZE,
        epochs=EPOCHS,
        validation_data=val_generator,
        validation_steps=val_generator.samples // BATCH_SIZE,
        callbacks=[early_stop, reduce_lr],
        verbose=1
    )

    # Fine-tuning: Unfreeze some layers
    print("\n🔧 Starting fine-tuning...")

    # Unfreeze the top layers of the base model
    base_model.trainable = True
    for layer in base_model.layers[:100]:
        layer.trainable = False

    # Recompile with lower learning rate
    model.compile(
        optimizer=Adam(learning_rate=0.0001/10),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )

    # Fine-tune for a few more epochs
    fine_tune_epochs = 10
    total_epochs = len(history.history['accuracy']) + fine_tune_epochs

    history_fine = model.fit(
        train_generator,
        steps_per_epoch=train_generator.samples // BATCH_SIZE,
        epochs=total_epochs,
        initial_epoch=len(history.history['accuracy']),
        validation_data=val_generator,
        validation_steps=val_generator.samples // BATCH_SIZE,
        callbacks=[early_stop, reduce_lr],
        verbose=1
    )

    # Combine histories
    for key in history.history.keys():
        history.history[key].extend(history_fine.history[key])

    # Save the model
    model_save_path = f"{base_path}/models/waste_classifier.h5"
    os.makedirs(os.path.dirname(model_save_path), exist_ok=True)
    model.save(model_save_path)
    print(f"✅ Model saved to: {model_save_path}")

    return model, history, train_generator.class_indices

def plot_training_history(history):
    """
    Plot training history
    """
    plt.figure(figsize=(12, 4))

    # Plot accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Plot loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.tight_layout()
    plt.savefig('/content/drive/MyDrive/DATA/training_history.png')
    plt.show()

if __name__ == "__main__":
    # Train the model
    model, history, class_indices = train_waste_classifier()

    # Plot training history
    plot_training_history(history)

    # Print final metrics
    final_train_acc = history.history['accuracy'][-1]
    final_val_acc = history.history['val_accuracy'][-1]

    print(f"\n🎯 FINAL TRAINING RESULTS:")
    print("=" * 30)
    print(f"📈 Final Training Accuracy: {final_train_acc:.4f} ({final_train_acc*100:.2f}%)")
    print(f"📊 Final Validation Accuracy: {final_val_acc:.4f} ({final_val_acc*100:.2f}%)")
    print(f"🎯 Class mapping: {class_indices}")

import pip install fastai

import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import os

def evaluate_waste_model():
    """
    Evaluate the trained waste classification model
    """
    print("🔍 EVALUATING MODEL PERFORMANCE")
    print("=" * 50)

    # Set paths
    base_path = "/content/drive/MyDrive/DATA./data"
    model_path = f"{base_path}/models/waste_classifier.h5"
    test_path = f"{base_path}/processed/testing"

    # Check if model exists
    if not os.path.exists(model_path):
        print(f"❌ Model not found at: {model_path}")
        print("Please train the model first!")
        return

    # Load the trained model
    print("📦 Loading trained model...")
    model = load_model(model_path)
    print("✅ Model loaded successfully!")

    # Test data generator
    test_datagen = ImageDataGenerator(rescale=1./255)

    test_generator = test_datagen.flow_from_directory(
        test_path,
        target_size=(224, 224),
        batch_size=32,
        class_mode='categorical',
        shuffle=False  # Important for correct label order
    )

    # Get true labels and predictions
    print("\n🎯 Making predictions on test set...")
    y_true = test_generator.classes
    y_pred_prob = model.predict(test_generator)
    y_pred = np.argmax(y_pred_prob, axis=1)

    # Calculate accuracy
    test_accuracy = np.sum(y_pred == y_true) / len(y_true)
    print(f"📊 Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)")

    # Classification report
    class_names = list(test_generator.class_indices.keys())
    print("\n📈 Classification Report:")
    print(classification_report(y_true, y_pred, target_names=class_names))

    # Confusion matrix
    plt.figure(figsize=(8, 6))
    cm = confusion_matrix(y_true, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names)
    plt.title('Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()
    plt.savefig(f'{base_path}/confusion_matrix.png')
    plt.show()

    # Test on individual samples
    print("\n🧪 Testing on sample images...")
    test_sample_images(model, test_path, class_names)

def test_sample_images(model, test_path, class_names):
    """
    Test the model on individual sample images
    """
    from tensorflow.keras.preprocessing import image
    import random

    # Test a few random images from each category
    for category in class_names:
        category_path = os.path.join(test_path, category)
        if os.path.exists(category_path):
            images = [f for f in os.listdir(category_path)
                     if f.lower().endswith(('.png', '.jpg', '.jpeg'))]

            if images:
                # Pick 2 random images
                sample_images = random.sample(images, min(2, len(images)))

                for img_name in sample_images:
                    img_path = os.path.join(category_path, img_name)

                    # Load and preprocess image
                    img = image.load_img(img_path, target_size=(224, 224))
                    img_array = image.img_to_array(img)
                    img_array = np.expand_dims(img_array, axis=0)
                    img_array /= 255.0

                    # Make prediction
                    prediction = model.predict(img_array)
                    predicted_class = class_names[np.argmax(prediction)]
                    confidence = np.max(prediction)

                    print(f"   📸 {category}/{img_name}:")
                    print(f"      🎯 Predicted: {predicted_class}")
                    print(f"      📊 Confidence: {confidence:.4f}")
                    print(f"      ✅ Correct: {predicted_class == category}")

if __name__ == "__main__":
    evaluate_waste_model()

from google.colab import files

# Download your trained model
files.download('/content/drive/MyDrive/DATA./data/models/waste_classifier.h5')

# Also download requirements.txt
with open('requirements.txt', 'w') as f:
    f.write('''streamlit>=1.28.0
tensorflow>=2.13.0
pillow>=9.0.0
numpy>=1.24.0
matplotlib>=3.7.0
opencv-python-headless>=4.8.0''')

files.download('requirements.txt')

!pip install streamlit

import streamlit as st
import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing import image
import numpy as np
from PIL import Image
import os

# Set page configuration
st.set_page_config(
    page_title="SmartWasteAI - Waste Classification",
    page_icon="🗑️",
    layout="centered"
)

# Load the trained model (with caching to avoid reloading on every interaction)
@st.cache_resource
def load_waste_model():
    try:
        model = load_model('/content/drive/MyDrive/DATA./data/models/waste_classifier.h5')
        return model
    except Exception as e:
        st.error(f"Error loading model: {e}")
        return None

# Load model
model = load_waste_model()

# Define class labels and their corresponding bin colors
class_labels = {
    0: {'name': 'Biodegradable', 'bin_color': 'Green', 'bin_emoji': '🟢',
        'bin_info': 'For organic waste like food scraps, garden waste, and biodegradable materials.'},
    1: {'name': 'Hazardous', 'bin_color': 'Red', 'bin_emoji': '🔴',
        'bin_info': 'For dangerous materials like batteries, chemicals, medical waste, and electronics.'},
    2: {'name': 'Recyclable', 'bin_color': 'Blue', 'bin_emoji': '🔵',
        'bin_info': 'For materials that can be recycled like paper, plastic, glass, metal, and textiles.'}
}

# Function to preprocess the uploaded image
def preprocess_image(img):
    img = img.resize((224, 224))  # Resize to match model's expected input
    img_array = image.img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)  # Create batch dimension
    img_array /= 255.0  # Rescale pixel values
    return img_array

# Function to make prediction
def predict_waste(img_array):
    if model is None:
        return None, 0.0

    predictions = model.predict(img_array, verbose=0)
    predicted_class = np.argmax(predictions[0])
    confidence = np.max(predictions[0])
    return predicted_class, confidence

# Streamlit UI
st.title("♻️ SmartWasteAI")
st.markdown("### AI-Powered Waste Segregation for Smart Cities")
st.write("Upload an image of a waste item, and our AI will classify it and recommend the correct disposal bin.")

# File uploader
uploaded_file = st.file_uploader("Choose an image...", type=["jpg", "jpeg", "png"])

if uploaded_file is not None:
    # Display the uploaded image
    image_display = Image.open(uploaded_file)
    st.image(image_display, caption="Uploaded Image", use_column_width=True)

    # Preprocess and predict
    with st.spinner('🔍 Analyzing the waste item...'):
        processed_image = preprocess_image(image_display)
        predicted_class, confidence = predict_waste(processed_image)

    if predicted_class is not None:
        # Get class info
        class_info = class_labels[predicted_class]

        # Display results
        st.success("✅ Analysis Complete!")

        # Create columns for layout
        col1, col2 = st.columns(2)

        with col1:
            st.metric(label="**Predicted Waste Type**",
                     value=f"{class_info['bin_emoji']} {class_info['name']}")
            st.metric(label="**Confidence Level**",
                     value=f"{confidence:.2%}")

        with col2:
            # Display bin recommendation
            st.markdown(f"### 🗑️ Recommended Bin: **{class_info['bin_color']} Bin** {class_info['bin_emoji']}")
            st.info(class_info['bin_info'])

        # Confidence bar
        st.markdown("### 📊 Confidence Level")
        st.progress(float(confidence))
        st.write(f"Model is {confidence:.2%} confident about this prediction")

    else:
        st.error("❌ Could not process the image. Please try another image.")

# Add a sidebar with information
with st.sidebar:
    st.header("ℹ️ About SmartWasteAI")
    st.write("This intelligent system uses deep learning to classify waste into three categories:")

    st.markdown("""
    **🟢 Green Bin - Biodegradable Waste**
    - Food scraps
    - Garden waste
    - Organic materials

    **🔵 Blue Bin - Recyclable Waste**
    - Paper & cardboard
    - Plastic containers
    - Glass bottles
    - Metal cans
    - Textiles

    **🔴 Red Bin - Hazardous Waste**
    - Batteries
    - Electronics
    - Chemicals
    - Medical waste
    """)

    st.markdown("---")
    st.write("**🔧 Built with:**")
    st.write("- TensorFlow & Keras")
    st.write("- MobileNetV2 (Transfer Learning)")
    st.write("- Streamlit")
    st.write("---")
    st.write("**🎓 Educational Project**")
    st.write("Machine Learning & Deep Learning Course")

# Footer
st.markdown("---")
st.markdown(
    "Developed as part of the **AI in Action Project** | "
    "**Machine Learning & Deep Learning Course**"
)